
Project title:
Microbiome dataÂ analysis of Zygnema green algae

Please read the instruction before you running commands.
We'll use this protocol command text file to provide all the commands and scripts for you (to avoid the ppt/word character formatting issue based on the previous experience).
All commands and scripts will be saved here.

The purpose of this project: 

Learn how to use raw metagenome sequencing data (fastq file/files) to do read cleaning, assembly/binning, and finally get MAGs for analyzing protein sequences, annotations, and functions.

For different programs, you would have two ways to run those:

1, use the preinstalled programs on hcc, which is convenient for beginners. However, hcc may not have the latest versions, and may have no newly published programs. You could search which programs are available on hcc website: https://hcc.unl.edu/docs/applications/modules/available_software_for_swan/. You could ask HCC to install new programs that you need: https://hcc.unl.edu/software-installation-request

2, I would recommand you using conda/mamba when you are more experienced. You could create your own software management environment on HCC and install your programs in that environment. Mamba is almost the same thing with Conda but much faster.

#part0, raw data

#in this project, we'll work on illumina pair-end metagenomic sequencing data of algae (Zygnematophyceae). It contains two raw read files: fastq1 and fastq2. The data consists of reads from both (host) algae and microbiome. We'll remove the algae data first and then focus on analyzing the microbiome data.

#cp the read data folder by following steps:

cd $WORK
mkdir final_project
cd final_project
cp -r /work/yinlab/yixing/course_project2/project_raw_data .


#part1, data preprocessing (~ 1hrs)

#We need check the quality of sequencing data first, and trim low quality data to get better results.
#we will use fastqc to do the quality check, and use trim_galore to do the trim.

#1. quality check before trim (~5-10 mins)

mkdir part1_data_preprocessing
cd part1_data_preprocessing
mkdir prefastqc
cd prefastqc
ml fastqc
fastqc ../../project_raw_data/MZCH241_bacteria.R_1.fastq --outdir .
fastqc ../../project_raw_data/MZCH241_bacteria.R_2.fastq --outdir .

#you will get two pair of results: zip&html, zip file is the details of quality, and html is the visulization of result.
#Download the html file to your laptop (use HCC ondemand) and see the results.
#Report the results of quality check, and explain the meaning of each plot.

#2. After that, we would delete those low quality reads by trimming using trim_galore (15 mins, depends the cores you used).

#Remember to do module unload your_previous_program, because sometimes different program may cause conflicts.
#won't remind you again.
#for the long time progress, we will offer you slurms to submit jobs on hcc, use nano to edit and save, and then sbatch.

cd ..
mkdir trim
cd trim

#!/bin/bash
#SBATCH --job-name=trim
#SBATCH --time=124:00:00
#SBATCH --mem=100gb
#SBATCH --partition=batch,guest (reminder: you could add your own partition, i.e. benson or yinlab)
#SBATCH --output=trim.%J.out
#SBATCH --error=trim.%J.err
#SBATCH --ntasks=20	
#SBATCH --cpus-per-task=1

module unload fastqc
module load trim_galore/0.6
trim_galore --paired your/path/of/fastq1.gz_file  your/path/of/fastq2.gz_file  -j 20

# sbatch the job, you would get two trim reports, one for each fastq file.


#3. Use fastqc again to check the quality again, and report the difference. You will get the trimmed read fastq files.
cd ..
mkdir postfastqc
cd postfastqc
# Run fastqc again

#part2, remove algae genes from raw reads (~ 1-2 hrs).
We want to analyze the microbiome data not the host data, so we need to remove those. Therefore, we need to prepare the reference genome file, which means the genomes we would like to remove, and use tools called Bowtie2 and samtools to do that. Here our reference genomes include algal nuclear genomes, plastid genomes, and mitogenomes sequencing by our lab. We'll then make index file for read mapping to the reference genomes. The algae_reference_genome.fna will be the reference genome file.

cd ../..
mkdir part2_remove_algae_genomes
cd part2_remove_algae_genomes
cp -r /work/yinlab/yixing/course_project2/project_raw_data/reference_genome .
cat * > algae_reference_genome.fna 



#!/bin/bash
#SBATCH --job-name=Bowtie2
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH --time=168:00:00
#SBATCH --mem=30gb
#SBATCH --output=Bowtie2.%J.out
#SBATCH --error=Bowtie2.%J.err
#SBATCH --partition=yinlab,batch,guest

ml bowtie/2.5

ml samtools/1.9	

bowtie2-build your_reference_genome_file index_prefix

bowtie2 -x index_prefix  -1 your/path/of/trimmed_fastq1_file -2 your/path/of/trimmed_fastq2_file -S bowtie2_alignments.sam --local -p $SLURM_NTASKS_PER_NODE

samtools view -bS -@ 16 bowtie2_alignments.sam > bowtie2_alignments.bam

samtools sort -@ 16 bowtie2_alignments.bam -o bowtie2_alignments.sorted.bam

samtools index bowtie2_alignments.sorted.bam

samtools view -b -f 12 -F 256 -@ 16 bowtie2_alignments.sorted.bam > unmapped.bam

samtools fastq -@ 16 -1 unmapped_1.fastq -2 unmapped_2.fastq unmapped.bam


# you will get two fastq files which contains reads unmapped to the algae genomes (i.e., de-contaminated reads).
#check the outputs and find the meaning of those files, report those.


 
#part3, assmebly with filtered reads (We will use two different tools : Megahit with ~40mins and MetaSPAde with ~8hrs).
#you will compare the results between two tools.

mkdir part3_assmebly
cd part3_assmebly

#!/bin/bash
#SBATCH --job-name=megahit
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=32
#SBATCH --time=168:00:00
#SBATCH --mem=250gb
#SBATCH --output=megahit.%J.out
#SBATCH --error=megahit.%J.err
#SBATCH --partition=yinlab,batch,guest

ml megahit/1.2

megahit  -1 your/path/to/your/previous/unmapped_1.fastq -2 your/path/to/your/previous/unmapped_2.fastq -o megahit_result -t 32



#!/bin/bash
#SBATCH --job-name=metaspade
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=32
#SBATCH --time=168:00:00
#SBATCH --mem=250gb
#SBATCH --output=metaspade.%J.out
#SBATCH --error=metaspade.%J.err
#SBATCH --partition=yinlab,batch,guest

ml spades/py35/3.13	

spades.py --meta -1 unmapped_1.fastq -2 unmapped_2.fastq -o metaspade_result --threads 32


#when the jobs are done, you would get two folders:
#for the metaSPAde, it contains two results: contigs.fasta and scaffolds.fasta; we'll use contigs.fa to continue.
#for the  megahit_result, we'll use final.contigs.fa.
#you can compare the difference between two contig files (number, length, etc.).
#for this project, we recommend to use megahit result for the all the following analyses.
#but you are free to choose metaSPAde result, or do both



#part4, bin contigs to make bins (MAGs) based on metawrap(~2.5 hrs).
#your_assembly_contig_fasta_file is the file name of whichever assembly file you will use (contigs.fasta from metaSPAde or final.contigs.fa from megahit).
mkdir part4_binning
cd part4_binning

#!/bin/bash
#SBATCH --job-name=metawrap
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH --time=168:00:00
#SBATCH --mem=250gb
#SBATCH --output=metawrap.%J.out
#SBATCH --error=metawrap.%J.err
#SBATCH --partition=yinlab,batch,guest

ml metawrap/1.3

metawrap binning -o BINNING_megahit -t 16 -a your_assembly_contig_fasta_file --metabat2 --maxbin2 --concoct your_fastq1file_removed_algae_genes your_fastq2file_removed_algae_genes -t 16

metawrap bin_refinement -o BIN_REFINEMENT_megahit -t 32 -A BINNING_megahit/metabat2_bins/ -B BINNING_megahit/maxbin2_bins/ -C BINNING_megahit/concoct_bins/

#In the metawrap, it combines three different method to do the bins, and bin_refinement is to combine those results together and summarize to give the final result.

#Compare those results and report. 
#the final results are in BIN_REFINEMENT_megahit/metawrap_70_10_bins/*.fa, each of those means a MAG.


#part5, check the quality of bins by CheckM2.

mkdir part5_checkm2
cd part5_checkm2

#!/bin/bash
#SBATCH --job-name=checkm2
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH --time=168:00:00
#SBATCH --mem=250gb
#SBATCH --output=checkm2.%J.out
#SBATCH --error=checkm2.%J.err
#SBATCH --partition=yinlab,batch,guest

ml checkm2/1.0


checkm2 predict --threads 16 --input ../part4_binning/BIN_REFINEMENT_megahit/metawrap_70_10_bins/*.fa  --output-directory checkm2_result

#report the results

#part6, taxonomic annotation by GTDB (~1-2hrs).

mkdir part5_taxonomic_annotation
cd part5_taxonomic_annotation

#!/bin/bash
#SBATCH --job-name=gtdb
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH --time=168:00:00
#SBATCH --mem=250gb
#SBATCH --output=gtdb.%J.out
#SBATCH --error=gtdb.%J.err
#SBATCH --partition=yinlab,batch,guest

ml gtdbtk/1.5 

gtdbtk classify_wf --genome_dir ../part4_binning/BIN_REFINEMENT_megahit/metawrap_70_10_bins/ --out_dir GTDB/ --cpus 16 --extension .fa

#you need to report the taxonomic assignment for each MAG from GTDB.


#part7, use DRAM for MAG functional annotation.

#!/bin/bash
#SBATCH --job-name=dram
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH --time=168:00:00
#SBATCH --mem=250gb
#SBATCH --output=dram.%J.out
#SBATCH --error=dram.%J.err
#SBATCH --partition=yinlab,batch,guest

ml dram/1.2

DRAM.py annotate -i '../bining/BIN_REFINEMENT_megahit/metawrap_70_10_bins/*.fa' -o annotation
DRAM.py distill -i annotation/annotations.tsv -o distill --trna_path annotation/trnas.tsv --rrna_path annotation/rrnas.tsv



#report the results of DRAM.









